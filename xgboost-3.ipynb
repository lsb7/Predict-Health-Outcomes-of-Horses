{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from scipy.stats import randint, uniform"
      ],
      "metadata": {
        "id": "Hz2vg1r9uQgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9vQvo8lt3LD",
        "outputId": "cd9c4c6d-2d8f-4fca-9690-6cdc1ab40d61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path = '/content/gdrive/MyDrive/Lim_ShiBin_ML/'\n",
        "os.makedirs(directory_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "bI5Tt3-Dt93C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/gdrive/MyDrive/Lim_ShiBin_ML/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-WnawBRuG68",
        "outputId": "8475b917-d8aa-49cc-a4e7-a7d1391d1944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Lim_ShiBin_ML\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "CF4H9foULb56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/content/gdrive/MyDrive/Lim_ShiBin_ML/train.csv')\n",
        "test_data = pd.read_csv('/content/gdrive/MyDrive/Lim_ShiBin_ML/test.csv')\n",
        "\n",
        "# # Feature engineering\n",
        "# def engineer_features(df):\n",
        "#     df = df.copy()\n",
        "\n",
        "#     # Create combinations of important features\n",
        "#     df['temp_pulse_ratio'] = df['rectal_temp'] / df['pulse']\n",
        "#     df['temp_resp_ratio'] = df['rectal_temp'] / df['respiratory_rate']\n",
        "#     df['pulse_resp_ratio'] = df['pulse'] / df['respiratory_rate']\n",
        "\n",
        "#     # Create interaction terms\n",
        "#     df['protein_ratio'] = df['total_protein'] / df['packed_cell_volume']\n",
        "\n",
        "#     # Bin continuous variables\n",
        "#     df['temp_category'] = pd.qcut(df['rectal_temp'], q=5, labels=['very_low', 'low', 'normal', 'high', 'very_high'], duplicates='drop')\n",
        "#     df['pulse_category'] = pd.qcut(df['pulse'], q=5, labels=['very_low', 'low', 'normal', 'high', 'very_high'], duplicates='drop')\n",
        "\n",
        "#     return df\n",
        "\n",
        "# # Apply feature engineering\n",
        "# train_data = engineer_features(train_data)\n",
        "# test_data = engineer_features(test_data)\n",
        "\n",
        "# Splitting features and target variable in training data\n",
        "X_train = train_data.drop(columns=[\"id\", \"hospital_number\", \"outcome\"])\n",
        "y_train = train_data[\"outcome\"]\n",
        "\n",
        "# Preprocessing test data (no target column)\n",
        "X_test = test_data.drop(columns=[\"id\", \"hospital_number\"])\n",
        "\n",
        "# Identifying numerical and categorical columns\n",
        "num_features = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "cat_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Preprocessing pipelines for numerical and categorical features\n",
        "num_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "cat_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combining preprocessors\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', num_transformer, num_features),\n",
        "        ('cat', cat_transformer, cat_features)])\n",
        "\n",
        "# Label encoding for the target variable in training data\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)"
      ],
      "metadata": {
        "id": "vGn6-6P9LfLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest  \n",
        "Public Score: 0.78658 (741/1,543)"
      ],
      "metadata": {
        "id": "COwL4EwiE8Ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Building the model pipeline\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Complete pipeline with preprocessor and model\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', rf)])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Predict on the test dataset\n",
        "y_test_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Decode the predicted labels back to the original categories\n",
        "y_test_pred_labels = label_encoder.inverse_transform(y_test_pred)\n",
        "\n",
        "# Convert predictions to DataFrame for easy inspection\n",
        "test_predictions = pd.DataFrame({\n",
        "    \"id\": test_data[\"id\"],\n",
        "    \"predicted_outcome\": y_test_pred_labels\n",
        "})\n",
        "\n",
        "test_predictions.to_csv(\"test_predictions_rf.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s37yV2lg3n0i",
        "outputId": "997f8a93-3718-4887-e7be-07f6c40acf4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample predictions:\n",
            "      id predicted_outcome\n",
            "0  1235             lived\n",
            "1  1236              died\n",
            "2  1237             lived\n",
            "3  1238        euthanized\n",
            "4  1239             lived\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting (XGBoost)  \n",
        "Public Score: 0.79878 (568/1,543)"
      ],
      "metadata": {
        "id": "0mORdqzyENSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Build model pipeline\n",
        "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                        ('classifier', XGBClassifier(\n",
        "                            n_estimators=100,\n",
        "                            use_label_encoder=False,\n",
        "                            eval_metric='mlogloss',\n",
        "                            random_state=42))])\n",
        "\n",
        "# Training model\n",
        "model.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Predict\n",
        "y_test_pred = model.predict(X_test)\n",
        "y_test_pred_labels = label_encoder.inverse_transform(y_test_pred)\n",
        "\n",
        "# Convert predictions to DataFrame for easier inspection\n",
        "test_predictions = pd.DataFrame({\n",
        "    \"id\": test_data[\"id\"],\n",
        "    \"predicted_outcome\": y_test_pred_labels\n",
        "})\n",
        "\n",
        "test_predictions.to_csv(\"test_predictions.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-WLRvlyuYlb",
        "outputId": "a201214e-a202-42b7-e6ad-334075b6d0da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [06:18:59] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample predictions:\n",
            "      id predicted_outcome\n",
            "0  1235             lived\n",
            "1  1236              died\n",
            "2  1237             lived\n",
            "3  1238        euthanized\n",
            "4  1239             lived\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting (XGBoost) with Hyperparameter Tuning and Cross-validation  \n",
        "Public Score: 0.82317 (314/1,543)"
      ],
      "metadata": {
        "id": "OBTK18MEFwRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "\n",
        "# Define hyperparameter grid for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'classifier__n_estimators': randint(50, 300),          # Number of boosting rounds\n",
        "    'classifier__max_depth': randint(3, 15),               # Depth of the tree\n",
        "    'classifier__learning_rate': uniform(0.01, 0.3),       # Learning rate\n",
        "    'classifier__subsample': uniform(0.5, 0.5),            # Fraction of samples used per boosting round\n",
        "    'classifier__colsample_bytree': uniform(0.5, 0.5),     # Fraction of features used per tree\n",
        "    'classifier__min_child_weight': randint(1, 10)         # Minimum sum of instance weight for child nodes\n",
        "}\n",
        "\n",
        "# Complete pipeline with preprocessor and model\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', xgb)])\n",
        "\n",
        "# Randomized search with cross-validation\n",
        "random_search = RandomizedSearchCV(\n",
        "    pipeline, param_distributions=param_dist, n_iter=100, scoring='accuracy', cv=5, verbose=2, n_jobs=-1, random_state=42\n",
        ")\n",
        "\n",
        "# Fit the model with RandomizedSearchCV\n",
        "random_search.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Display best parameters and accuracy score\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "print(\"Best cross-validation accuracy: \", random_search.best_score_)\n",
        "\n",
        "# Use the best model found in RandomizedSearchCV to make predictions\n",
        "best_xgb_model = random_search.best_estimator_\n",
        "y_test_pred = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Decode the predicted labels back to their original categories\n",
        "y_test_pred_labels = label_encoder.inverse_transform(y_test_pred)\n",
        "\n",
        "# Convert predictions to DataFrame for easy inspection\n",
        "test_predictions = pd.DataFrame({\n",
        "    \"id\": test_data[\"id\"],\n",
        "    \"predicted_outcome\": y_test_pred_labels\n",
        "})\n",
        "\n",
        "test_predictions.to_csv(\"test_predictions_xgb.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw4z82Z43fBG",
        "outputId": "ca91275f-5f9e-4565-f442-b2f401d1f29c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [16:36:55] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'classifier__colsample_bytree': 0.3936964831604432, 'classifier__learning_rate': 0.015468785930798914, 'classifier__max_depth': 8, 'classifier__min_child_weight': 12, 'classifier__n_estimators': 296, 'classifier__subsample': 0.9998588366430653}\n",
            "Best cross-validation accuracy:  0.7206477732793524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define base estimators\n",
        "estimators = [\n",
        "    ('xgb', XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42, use_label_encoder=False, eval_metric='mlogloss')),\n",
        "    ('lgbm', LGBMClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)),\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42))\n",
        "]\n",
        "\n",
        "# Define the stacking ensemble with a Logistic Regression meta-model\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(),\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "stacking_clf.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_test_pred = stacking_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(\"Stacking Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "byaJLrZL5kNd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}